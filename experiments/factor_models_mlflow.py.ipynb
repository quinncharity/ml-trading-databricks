{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889c8aec-e3ec-469c-9545-d703140a4143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import mlflow\n",
    "import mlflow.spark\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.functions import vector_to_array\n",
    "from mlflow.exceptions import MlflowException\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e91c5cc-bcac-42e2-ba32-b4673845991f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# MLflow experiment configuration\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# High-level parameters for the trading setup\n",
    "cutoff_date = \"2020-01-01\"   # train/test split\n",
    "lookahead_days = 5           # matches the 5-day lookahead in feature engineering\n",
    "long_threshold = 0.55        # P(up) threshold for long\n",
    "short_threshold = 0.45       # P(up) threshold for short\n",
    "\n",
    "feature_cols = [\"sma_20\", \"std_20\", \"daily_return\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0d7af76-fd4c-4913-b98e-d128bb4f471e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Data loading\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "df = spark.table(\"market.features_labeled\")\n",
    "\n",
    "df_clean = df.dropna(subset=feature_cols + [\"label\"])\n",
    "\n",
    "train = df_clean.filter(F.col(\"Date\") < cutoff_date)\n",
    "test = df_clean.filter(F.col(\"Date\") >= cutoff_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01864a4-7f8a-4d12-9432-c4987cff6028",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Helpers\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def compute_sharpe(pdf: pd.DataFrame, ret_col: str = \"portfolio_return\") -> float | None:\n",
    "    \"\"\"\n",
    "    Compute annualized Sharpe ratio from a pandas DataFrame with a return column.\n",
    "    Assumes daily returns.\n",
    "    \"\"\"\n",
    "    mean_ret = pdf[ret_col].mean()\n",
    "    std_ret = pdf[ret_col].std()\n",
    "\n",
    "    if std_ret and std_ret != 0:\n",
    "        return (mean_ret / std_ret) * (252 ** 0.5)\n",
    "    return None\n",
    "\n",
    "\n",
    "def backtest_from_predictions(preds_sel):\n",
    "    \"\"\"\n",
    "    preds_sel: Spark DataFrame with columns:\n",
    "      Date, symbol, label, prediction, probability, daily_return\n",
    "\n",
    "    Returns a Spark DataFrame with:\n",
    "      Date, portfolio_return, benchmark_return, cml_return, benchmark_cml_return\n",
    "    \"\"\"\n",
    "    # Build trading strategy DataFrame\n",
    "    strategy_df = (\n",
    "        preds_sel\n",
    "        .withColumn(\"prob_array\", vector_to_array(\"probability\"))\n",
    "        .withColumn(\"p_up\", F.col(\"prob_array\")[1])  # P(class = 1)\n",
    "        .withColumn(\n",
    "            \"position\",\n",
    "            F.when(F.col(\"p_up\") > long_threshold, 1)\n",
    "             .when(F.col(\"p_up\") < short_threshold, -1)\n",
    "             .otherwise(0)\n",
    "        )\n",
    "        .withColumn(\"strategy_return\", F.col(\"position\") * F.col(\"daily_return\"))\n",
    "    )\n",
    "\n",
    "    # Aggregate to daily portfolio returns\n",
    "    portfolio_df = (\n",
    "        strategy_df\n",
    "        .groupBy(\"Date\")\n",
    "        .agg(F.avg(\"strategy_return\").alias(\"portfolio_return\"))\n",
    "        .orderBy(\"Date\")\n",
    "    )\n",
    "\n",
    "    # Equal-weight benchmark on daily_return across all symbols\n",
    "    benchmark_df = (\n",
    "        df_clean\n",
    "        .groupBy(\"Date\")\n",
    "        .agg(F.avg(\"daily_return\").alias(\"benchmark_return\"))\n",
    "        .orderBy(\"Date\")\n",
    "    )\n",
    "\n",
    "    # Combine strategy and benchmark, compute cumulative returns\n",
    "    comparison_df = (\n",
    "        portfolio_df.alias(\"p\")\n",
    "        .join(benchmark_df.alias(\"b\"), \"Date\", \"inner\")\n",
    "        .select(\n",
    "            \"Date\",\n",
    "            \"portfolio_return\",\n",
    "            \"benchmark_return\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    w_date = Window.orderBy(\"Date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    comparison_df = (\n",
    "        comparison_df\n",
    "        .withColumn(\"cml_return\", F.sum(\"portfolio_return\").over(w_date))\n",
    "        .withColumn(\"benchmark_cml_return\", F.sum(\"benchmark_return\").over(w_date))\n",
    "    )\n",
    "\n",
    "    return comparison_df\n",
    "\n",
    "\n",
    "def get_git_sha() -> str | None:\n",
    "    try:\n",
    "        return (\n",
    "            subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"])\n",
    "            .decode(\"utf-8\")\n",
    "            .strip()\n",
    "        )\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "088a46a3-a2f6-4578-8a9b-f770e50783a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Core MLflow experiment function\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def run_experiment(\n",
    "    reg_param: float,\n",
    "    elastic_net_param: float,\n",
    "    max_iter: int,\n",
    "    registered_model_name: str = \"ml_trading_lr_v1\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Run a single logistic regression experiment and log everything to MLflow.\n",
    "    \"\"\"\n",
    "\n",
    "    git_sha = get_git_sha()\n",
    "\n",
    "    with mlflow.start_run(\n",
    "        run_name=f\"logreg_r{reg_param}_en{elastic_net_param}_iter{max_iter}\"\n",
    "    ):\n",
    "        # Tags and params for lineage\n",
    "        mlflow.log_param(\"reg_param\", reg_param)\n",
    "        mlflow.log_param(\"elastic_net_param\", elastic_net_param)\n",
    "        mlflow.log_param(\"max_iter\", max_iter)\n",
    "        mlflow.log_param(\"lookahead_days\", lookahead_days)\n",
    "        mlflow.log_param(\"long_threshold\", long_threshold)\n",
    "        mlflow.log_param(\"short_threshold\", short_threshold)\n",
    "        mlflow.log_param(\"cutoff_date\", cutoff_date)\n",
    "\n",
    "        if git_sha:\n",
    "            mlflow.set_tag(\"git_commit\", git_sha)\n",
    "        mlflow.set_tag(\"dataset_table\", \"market.features_labeled\")\n",
    "        mlflow.set_tag(\"model_type\", \"logistic_regression\")\n",
    "        mlflow.set_tag(\"project\", \"ml_trading_databricks\")\n",
    "\n",
    "        # Build feature assembler and model\n",
    "        assembler = VectorAssembler(\n",
    "            inputCols=feature_cols,\n",
    "            outputCol=\"features\"\n",
    "        )\n",
    "\n",
    "        lr = LogisticRegression(\n",
    "            labelCol=\"label\",\n",
    "            featuresCol=\"features\",\n",
    "            regParam=reg_param,\n",
    "            elasticNetParam=elastic_net_param,\n",
    "            maxIter=max_iter,\n",
    "        )\n",
    "\n",
    "        dataset = assembler.transform(df_clean)\n",
    "\n",
    "        train_local = dataset.filter(F.col(\"Date\") < cutoff_date)\n",
    "        test_local = dataset.filter(F.col(\"Date\") >= cutoff_date)\n",
    "\n",
    "        model = lr.fit(train_local)\n",
    "\n",
    "        # Predictions\n",
    "        preds = model.transform(test_local)\n",
    "\n",
    "        preds_sel = preds.select(\n",
    "            \"Date\",\n",
    "            \"symbol\",\n",
    "            \"label\",\n",
    "            \"prediction\",\n",
    "            \"probability\",\n",
    "            \"daily_return\",\n",
    "        )\n",
    "\n",
    "        # Backtest\n",
    "        comparison_df = backtest_from_predictions(preds_sel)\n",
    "        pdf_compare = comparison_df.toPandas()\n",
    "\n",
    "        # Metrics\n",
    "        sharpe = compute_sharpe(pdf_compare, ret_col=\"portfolio_return\")\n",
    "        benchmark_sharpe = compute_sharpe(pdf_compare, ret_col=\"benchmark_return\")\n",
    "        mean_ret = pdf_compare[\"portfolio_return\"].mean()\n",
    "        std_ret = pdf_compare[\"portfolio_return\"].std()\n",
    "\n",
    "        mlflow.log_metric(\"sharpe\", sharpe if sharpe is not None else float(\"nan\"))\n",
    "        mlflow.log_metric(\n",
    "            \"benchmark_sharpe\",\n",
    "            benchmark_sharpe if benchmark_sharpe is not None else float(\"nan\"),\n",
    "        )\n",
    "        mlflow.log_metric(\"mean_daily_return\", float(mean_ret))\n",
    "        mlflow.log_metric(\"std_daily_return\", float(std_ret))\n",
    "\n",
    "        # Plot cumulative returns\n",
    "        fig = plt.figure(figsize=(12, 6))\n",
    "        plt.plot(pdf_compare[\"Date\"], pdf_compare[\"cml_return\"], label=\"ML strategy\")\n",
    "        plt.plot(\n",
    "            pdf_compare[\"Date\"],\n",
    "            pdf_compare[\"benchmark_cml_return\"],\n",
    "            label=\"Benchmark\",\n",
    "        )\n",
    "        plt.legend()\n",
    "        plt.title(\"Cumulative returns: ML strategy vs. benchmark\")\n",
    "        plt.tight_layout()\n",
    "\n",
    "        mlflow.log_figure(fig, \"cumulative_returns.png\")\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Log and register the model (may fail on shared/serverless clusters)\n",
    "        try:\n",
    "            mlflow.spark.log_model(\n",
    "                spark_model=model,\n",
    "                artifact_path=\"model\",\n",
    "                registered_model_name=registered_model_name,\n",
    "            )\n",
    "        except MlflowException as e:\n",
    "            print(\n",
    "                \"Skipping Spark model logging on this cluster. \"\n",
    "                \"Params/metrics/plots are still logged to MLflow. \"\n",
    "                f\"Reason: {e}\"\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7eba935-eff5-44d5-90ae-06bc14591531",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Hyperparameter sweep\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    configs = [\n",
    "        {\"reg_param\": 0.0, \"elastic_net_param\": 0.0, \"max_iter\": 50},\n",
    "        {\"reg_param\": 0.01, \"elastic_net_param\": 0.0, \"max_iter\": 50},\n",
    "        {\"reg_param\": 0.1, \"elastic_net_param\": 0.0, \"max_iter\": 50},\n",
    "        {\"reg_param\": 0.01, \"elastic_net_param\": 0.5, \"max_iter\": 100},\n",
    "    ]\n",
    "\n",
    "    for cfg in configs:\n",
    "        run_experiment(**cfg)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "factor_models_mlflow.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
